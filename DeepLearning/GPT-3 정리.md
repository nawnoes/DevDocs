# GPT-3 개요
![](https://images.velog.io/images/nawnoes/post/ae41e08b-fe5c-4f54-9c08-7528da5132d0/image.png)  

## 개요
최근에는 대규모의 텍스트를 가지고 사전학습 후 파인튜닝을 거치면서 여러 NLP 과제들에 대해서 좋은 성능을 얻었다. 하지만 인간의 경우를 생각해보면 적은 수의 예를 가지고 여러가지 과제들을 처리할 수 있는데, NLP 시스템은 여전히 많은 라벨링된 데이터를 필요로 한다. 이러한 부분은 최신 NLP 시스템과 사람의 차이는 여전히 나타낸다.  
  
따라서 GPT-3의 경우 이전의 `non-sparse언어 모델`보다 10배 큰 파라미터인 1750억개의 파라미터를 가진 모델을 사전학습 후 파인 튜닝 없이 `auto-regressive` 방식으로 학습 한 후에 그래디언트 업데이트나 파인튜닝 없이, `few-shot` 학습 방법을 통해 여러가지 NLP 태스크에 대해 강력한 성능을 얻었다. 몇가지 부분에 대해서 GPT-3의 few-shot 학습이 좋은 성능을 발휘하지 못하는 부분도 있지만, `기사` 작성에 대해서 인간 평가자들이 인간 기자와 GPT3의 차이를 발견하기 어려워 했다.
  
  **few-shot 학습 방법이란**  
  ![](https://images.velog.io/images/nawnoes/post/79357bc9-83a2-4e5a-ada4-25309bb4ebfd/image.png)
  `zero-shot`, `one-shot`, `few-shot`, `fine-tuning`의 여러 학습 방법 중의 하나로 `몇가지`의 학습 예를 바탕으로 학습을 하는것으로 보면 된다.  

  처음에는 몇가지의 학습 데이터로 학습한다는게 무슨 의미인지 처음에는 잘 몰랐다. GPT-3를 살펴보다 보니, 최근의 NLP 태스크들은 사전학습 후에 추가로 대용량의 라벨링된 데이터들을 파인튜닝하는 과정을 통해 해결하려고 하는데, 활용 측면으로 보았을 때 다양한 NLP 태스크들에 대해서 큰용량의 라벨링된 데이터를 얻는건 어려움이 있을 것이기 때문에 인간의 학습 방법과 유사하게 학습하기 위해 Few-Shot, Zero-Shot등의 학습이 대해 계속적으로 연구하는 것으로 보인다.


## Contents
1. Introduction
2. Approach
3. Result 
4. Measuring and Preventing Memorization Of Benchmarks 
5. Limitations
6. Broader Impacts
7. Related Work
8. Conclusion
etc. 기타 디테일한 사항들

## 접근방법

최근 자연어처리 시스템에서는 사전학습된 language representions에 파인튜닝 하는 과정을 거친다. 하지만 이러한 접근 방식들도 다운스트림 태스크를 학습하는데 있어 수천에서 수십만의 특정 데이터가 필요하다. 따라서 사전학습과 파인튜닝 패러다임의 아래 제한사항들을 극복 하는것이 필요하다.
  
  1. 실용적인 측면에서 모든 자연어 태스크에 대한 대용량의 라벨링된 데이터 세트가 필요하다는 점은 언어모델의 적용 가능성을 제한한다.
  
  2. 학습데이터에 대한 거짓 상관관계가 나타날 가능성은 모델의 표현과 학습 분포의 협소성과 함께 증가한다. **GPT3에서의 관점은 사전학습 다음에 파인튜닝을 거치는 것은 사전학습하는 동안 정보를 흡수하기 위해 크게 설계된 모델이 매우 협소한 태스크에 대해 파인튜닝 되는 것으로 본다**.  

  몇가지 사례를 통해 대형 모델 대형 모델들이 일반화를 위해 필수 사항이 아니고, 이러한 사전학습+파인튜닝 패러다임이 성능이 저조한 경우도 있다.
  
  3. 사람은 대부분의 자연어 태스크들을 학습하는데 많은 수의 데이터나 예가 필요로 하지 않다. 사람의 예를 생각해보면 긴 대화를 하는 동안 다양한 자연어 task들과 skill들을 매끄럽게 섞어서 사용하는것을 생각해보면 된다.
  
 ### 메타러닝 meta-learning
 ![](https://images.velog.io/images/nawnoes/post/2232e81d-049a-4829-972a-b02c08fb7001/image.png) 
 `in-context learning`이라 불리는 방법을 통해 학습된다. 비감독 사전학습 과정에서 언어모델이 광범위하게 skill과 패턴 인식 능력을 개발할 때, 이과정의 내부 루프를 설명하기 위해 `in-context-learning` 용어를 사용한다.  각각의 다이어그램은 학습 데이터의 전체 모습을 나타낸다는 것이 아닌, 단일 시퀀스 내에서 반복적으로 포함된 하위 태스크들(sub-task)들이 있음 보여준다.

> 그림을 해석하기로는 각 시퀀스들을 돌면서 시퀀스 안에 세자릿수 덧셈, 오탈자 교정, 번역등과 같이 데이터가 학습된다는 것으로 보인다. 컨텍스트 안에서 학습된다는 의미로 보인다
 
 위의 제한 사항들을 해결하기 위한 한가지 방법으로 메타 러닝이 있다. 메타 러닝은 언어 모델의 맥락 안에 모델이 학습을 거치는 동안 넓은 범위의 기술과 패턴을 인식하는 능력을 개발하는 것을 의미한다. 그리고 개발된 능력들은 예측시간(inference time)에 매우 빠르게 적용하거나 태스크들을 인식한다.
  
사전 학습된 언어모델의 텍스트 인풋을 task specification 의 한 형태로 이용하여 `in-context-learning`을 한 최근 사례도 있다: 모델을 자연어 지시 및/또는 몇가지 task demonstration들에 대한 조건이 주어지고, 모델은 다음 순서에 무엇이 올지를 예측하므로써 간단하게 주어진 인스턴스들을 완성하기를 기대된다.

이것이 초기에 가능성을 보이기도 했지만, 파인튜닝보다 훨씬 낮을 결과를 얻는다. `Natural Questions`에 대해서는 4%의 결과만 얻고, 다른 CoQa 태스크에서도 낮은 성능을 보였다. 따라서 메타 러닝이 자연어 처리에서 실제적으로 사용되기 위해서는 상당한 개선이 필요해보였다.

### 또 다른 방법, 모델의 크기
![](https://images.velog.io/images/nawnoes/post/5ec6df03-5852-495f-aabf-f795432bb38e/image.png)  
최근 트랜스포머의 크기는 상당히 증가했고, 1억개의 파라미터 부터 3억개의 파라미터를 지나, 15억개의 파라미터, 80억개, 110억개, 170억개의 파라미터로 증가했다. 각각의 증가는 성능향상을 이루었다. 그 증거로는 많은 다운스트림 태스크들에 대해 상관관계가 큰 log loss 가 `모델의 규모`에 따라 부드럽게 개선되는 경향이 있다.  
  
`in-context learning`은 통해 모델의 파라미터들이 많은 스킬들과 태스크들에 대해 학습하기 때문에, `모델의 규모`에 따라 강력한 성능을 보인다.

# GPT-3가 보이고 싶은 것

큰 모델을 in-context-learning을 하고, 다양한 태스크들에 대해 적은 수의 데이터만을 가지고 학습하는 능력을 가졌다고 가정하였다. GPT-3는 175억개의 파라미터를 `autoregressive language model`로 학습하므로써 더 큰 모델이 보다 앞서 보인 가정을 실험해보고자 했다.  
  
GPT-3의 경우 24개가 넘은 nlp 태스크들에 대해서 성능을 평가하고, 그 뿐만 아니라 학습에서 직접적으로 포함 되지 않은 태스크들에 대해서 얼마나 빠르게 적용될 수 있는지 테스트 하기 위해 여러 새로운 태스크들도 테스트 했다.  
  
GPT-3의 경우 3가지 조건 아래서 평가를 시행했다. NLP 태스크를 테스트하는데 있어서 어느정도 규모의 데이터로 학습하고 평가를 할것인가에 대한 부분을 나타낸다.
- **few-shot learning**: 10~ 100개 정도 예시를 사용
- **one-shot learning**: 단 1개의 예시 사용.
- **zero-shot learning**: 단 하나의 예시 없이 언어 모델을 바로 NLP 태스크에 테스트
> GPT-3로 파인튜닝을 통해 학습할 수도 있지만 이것을 미래에 맡겨둔다고 한다..
  
자연어처리 태스크에서 GPT-3는 zero-shot과 one-shot 학습에서 좋은 성능을 얻었으며, few-shot 학습에서는 때때로 **SOTA**에 비교해도 경쟁력을 가지는 경우도 있었다.  
  
GPT-3는 one-shot, few-shot 학습에서 GPT-3가 적은 수의 데이터에 대해서도 얼마나 빠르게 학습할 수 있는지 테스트 하기 위한 태스크 또는 즉석 추론에서 능숙함을 보여준다. 이때 즉석 추론이단 단 한번 나타난 단어를 보고 그 이후에 문장에서 새로운 단어를 사용하는것과 산수를 하는것을 말한다. 그리고 GPT-3는 사람이 구분하기 어려울만큼 뉴스 기사를 잘 생성해 낸다.  
  
동시에 GPT-3 모델의 크기에도 불구하고 few-shot 학습이 잘 동작하지 않는곳이 있다. 예를들면 NLI(Natural Language Inference), Reading Comprehension등에서 부족한 부분을 보인다. GPT-3의 강점과 약점을 보이면서 few-shot 학습에 대한 연구를 자극하길 원한다.  
  
![](https://images.velog.io/images/nawnoes/post/fc2bc6c2-3a26-4bb1-9f4a-55c34b40485f/image.png)  
GPT-3는 125M 크기의 모델부터 13B의 모델까지 학습시켜 테스트 했으며, 각각 zero, one, few shot 세팅에 대해 비교하였다. 

**_대체적으로 모델의 크기가 커질수록 제로샷, 원샷, 퓨샷 학습방법에 대해서 성능이 종종 오르는 패턴을 볼수 있고, 이것으로 볼때 큰 모델이 더 숙련된 meta-learner로 판단된다._**

이후에는 GPT-3가 접근한 방법과 메소드들을 다루고 GPT-3가 보여주는 능력의 범위를 고려하여, 편향, 공정, 사회적 영향들에 대해 논하고, 우려들에 대해 분석해본다. 

# GPT-3의 모델
## 접근 방법
GPT-3의 접근 방법은 기존의 GPT-2의 모델, 학습데이터, 학습 방법 등에서 유사하다. 거기서 확장하여, 모델의 사이즈와 데이터의 사이즈를 확대했으며, 데이터의 다양성을 증가했고, 학습 길이도 증가시켰다.  
  
`in-context learning` 또한 이전의 GPT-2와 유사하게 진행되었고, 학습에서 다양한 세팅을 체계적으로 찾는 실험을 하였다. 제로샷, 퓨샷, 원샷 학습을 진행하면서 각 nlp 태스크들이 얼마 데이터에 의존적인지도 알수 있다. 

## Setting
![](https://images.velog.io/images/nawnoes/post/023f2ad1-0c9f-4308-a246-eb5f5a99fc40/image.png)
GPT-3에서는 궁극적으로 원샷 또는 제로샷이 사람과 가장 공평한 비교로 생각하며, 앞으로 미래의 작업에 주요한 목표가 된다.

## Model & Architectures
![](https://images.velog.io/images/nawnoes/post/c38ffb14-e4ea-43e1-b7fc-5bded267e787/image.png) 위 모델들은 학습 시 사용되는 하이퍼 파라미터를 나타내며, 모든 모델은 300 Billion 토큰을 학습하는데 사용했다.
	- $n_{params}$: 학습 가능한 파라미터의 총 수
	- $n_{layers}$: 전체 레이어 수
	- $d_{model}$: bottleneck layer의 유닛수 
    	(GPT-3에서는 언제나 bottleneck layer의 4배 크기의 $feedforward$ 레이어를 사용한다. $d_{ff}$ = $4 * d_{model}$)
	- 모든 모델은 $n_{ctx}$ =2048의 context window를 사용
	- 각 노드간의 데이터 이동을 최소화하기 위해 **width**와 **depth**의 dimension에 따라 간에 모델을 나눈다.
	- $n_{params}$: 학습 가능한 파라미터의 총 수


### 모델
`GPT-3`에서는 GPT-2와 같은 모델과 구조를 사용했고, 수정된 initialization, pre-normalization, reversible tokenization들을 사용했다. 다만 **Transformer**레이어에서 $Sparse Transformer$와 유사한 alternating dense와 locally banded sparse attention을 사용했다. 그리고 8가지 크기의 모델사이즈에 대해 실험하였다. 이때 모델의 사이즈는 125M에서 175B까지 범위로 실험하였다. 

## Training Dataset
언어 모델의 데이터셋은 빠르게 확장 되고 있다. **Common Crawl** 데이터셋을 통해 약 1조 단어의 데이터가 축적 되었다. 1조 단어의 데이터 셋은 GPT-3의 큰 모델의 학습시키는데에 충분했다. 이 데이터셋은 동일한 시퀀스를 2번 업데이트 하지 않고도, 가장 큰 모델을 교육하기에 충분했다.  
  
하지만 **Common Crawl**데이터는 필터링 되지 않거나, 가볍게 필터링 되었기 때문에 정제된 데이터들 보다 낮은 퀄리티를 보였고, 그래서 3단계를 거쳐서 데이터 셋의 평균 퀄리티를 높이고자 했다.  
### 데이터셋 퀄리티를 높이기 위한 방법
1,2번의 **Common Crawl**에 대한 자세한 내용은 부록에 있다. 3번의 항목에 대해서는 고퀄리티 데이터를 추가하기 위해 여러가지 선별된 데이터셋을 추가했으며, $WebText$의 확장된 버전, 오랜기간 동안 스크래핑한 데이터, 인터넷 기반의 책(Book1, Book2), 영어 Wikipedia 들을 데이터로 사용했다. 

	1. 고품질의 참조 말뭉치 범위와 유사한 Common Crawl를 다운드하고 정제했다.
    2. Document Level에서 `fuzzy deduplication`을 수행함으로써 중복을 방지하고, 오버피팅의 정확한 측정으로써 validation set의 무결성을 보존한다.
    3. 유명한 고퀄리티 말뭉치들을 추가하여 다양성을 높인다.
    
 CommonCrawl의 경우 2016~2019년 기간의 데이터를 사용했으며 전처리 되기 전에는 45TB의 텍스트로 구성되어 있었고 전처리 후에는 570GB의 용량을 가진다. 
 
### 모델 별 학습하는데 사용되는 컴퓨팅
![](https://images.velog.io/images/nawnoes/post/130db040-d1cd-49f8-985b-8cf3fec27bfc/image.png)

### 학습하는데 사용되는 데이터  
...작성중...
    
# References
https://arxiv.org/pdf/2005.14165.pdf