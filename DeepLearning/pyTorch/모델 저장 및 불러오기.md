# 모델 저장 및 불러오기

## 모델을 저장하거나 불러올 때 사용하는 3가지 핵심 함수
1. torch.save: 직렬화된 객체를 디스크에 저장. 직렬화는 pickle을 사용. 모든 종류의 객체의 모델 및 텐서, 딕셔너리 저장 가능
2. torch.load: pickle을 사용하여 저장된 객체 파일들을 역직렬화를 통해 메모리에 올린다. 데이터를 장치에 불러올 때도 사용.
3. torch.nn.Module.load_state_dict: 역질렬화된 state_dict를 사용해서 모델의 매개변수를 불러온다. sta

## 1. 다른 모델의 매개변수를 사용하여 빠르게 모델 시작하기
모델의 일부를 불러와 전이 학습 또는 새로운 모델을 학습할때 일반적으로 사용하는 
**저장하기**
```python
torch.save(modelA.state_dict(), PATH)
```
  
**불러오기**
```python
modelB = TheModelBClass(*args, **kwargs)
modelB.load_state_dict(torch.load(PATH), strict=False)
```

## 2. 장치(Device)간 모델 저장 및 불러오기
### GPU -> CPU 불러 오기
**저장하기**
```python
torch.save(model.state_dict(), PATH)
```
  
**불러오기**
```python
device = torch.device('cpu')
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location=device))
```

### CPU -> GPU 불러 오기
**저장하기**
```python
torch.save(model.state_dict(), PATH)
```
  
**불러오기**
```python
device = torch.device("cuda")
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location="cuda:0"))  # 사용할 GPU 장치 번호를 선택합니다.
model.to(device)
# 모델에서 사용하는 input Tensor들은 input = input.to(device) 을 호출해야 합니다.
```

### torch.nn.DataParallel 모델 저장하기
torch.nn.DataParallel 은 병렬 GPU 활용을 가능하게 하는 모델 래퍼(wrapper)입니다. DataParallel 모델을 범용적으로 저장하려면 model.module.state_dict() 을 사용