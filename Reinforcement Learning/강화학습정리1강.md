# ê°•í™”í•™ìŠµì´
## ê°•í™”í•™ìŠµ ê°œìš”
### ë¨¸ì‹ ëŸ¬ë‹
ë¨¸ì‹ ëŸ¬ë‹ì˜ ì¢…ë¥˜ëŠ” í¬ê²Œ 3ê°€ì§€ê°€ ìˆë‹¤. ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµ.
  - ì§€ë„í•™ìŠµ: ì–´ë–¤ ì…ë ¥ê³¼ ì •ë‹µì´ ìˆëŠ” ë°ì´í„° ì„¸íŠ¸ë¥¼ ì´ìš©í•´ í•™ìŠµ
  - ë¹„ì§€ë„í•™ìŠµ: ì •ë‹µì´ ì—†ëŠ” ë°ì´í„° ì„¸íŠ¸ë¥¼ ì´ìš©í•´ í•™ìŠµ
  - ê°•í™”í•™ìŠµ: ì—ì´ì „íŠ¸ì™€ í™˜ê²½ì´ ìƒí˜¸ì‘ìš©ì„ í•˜ë©° ë³´ìƒì„ ë°›ëŠ” ê³¼ì •ì—ì„œ ìƒê¸´ ë³´ìƒì„ í†µí•´ í•™ìŠµí•˜ëŠ”ê²ƒ

### MDP(Markov Decision Process) 

### ê°•í™”í•™ìŠµì˜ ìš©ì–´
- ì—ì´ì „íŠ¸(agent): ê°•í™”í•™ìŠµì—ì„œ ì•¡ì…˜ì„ ê²°ì •í•˜ëŠ” ì£¼ì²´
- í™˜ê²½(environment): ì—ì´ì „íŠ¸ê°€ ê²°ì •í•œ ì•¡ì…˜ì„ ë°˜ì˜í•˜ê³ , ê·¸ ì •ë³´ë¥¼ ì—ì´ì „íŠ¸ì— ì œê³µ
- í–‰ë™(action, a): í™˜ê²½ì—ì„œ ìƒíƒœë¥¼ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” í–‰ë™
    * ì´ì‚°ì  í–‰ë™: ì—¬ëŸ¬ê°€ì§€ í–‰ë™ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ (ìƒ, í•˜, ì¢Œ, ìš° ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ëŠ” ê²½ìš°)
    * ì—°ì†ì  í–‰ë™: ì—°ì†ì ì¸ ê°’ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ (-1ê³¼ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ ê°’)
- ì—­ì‚¬
- ìƒíƒœ(state, s): ê´€ì¸¡, í–‰ë™, ë³´ìƒì„ ê°€ê³µí•œ ì •ë³´
- ê´€ì¸¡(observation): í™˜ê²½ì—ì„œ ì œê³µí•´ì£¼ëŠ” ì •ë³´
    * ì‹œê°ì  ê´€ì¸¡
    * ìˆ˜ì¹˜ì  ê´€ì¸¡
- ë³´ìƒ(reward, $R_t$): ìŠ¤ì¹¼ë¼ ê°’ì˜ í”¼ë“œë°±. ì—ì´ì „íŠ¸ëŠ” ë³´ìƒì˜ í•©ì„ ìµœëŒ€í™” í•˜ëŠ”ê²ƒì´ë‹¤. ê°•í™”í•™ìŠµì€ 
- ë³´ìƒê°€ì •(Reward Hypothesis): ê¸°ëŒ€ë˜ëŠ” ë³´ìƒì˜ ëˆ„ì ì„ ìµœëŒ€í™”í•˜ëŠ”ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ”ê²ƒ
- ë³´ìƒí•¨ìˆ˜: í˜„ì¬ ìƒíƒœ sì—ì„œ íŠ¹ì • í–‰ë™ aë¥¼ í–ˆì„ ë•Œ ì–»ëŠ” ë³´ìƒì˜ ê¸°ëŒ€ê°’ $R^a_s$  
    > $R^a_s = E[R_{t+1} | S_t = s, A_t = a]$

- ê°ê°€ìœ¨(Discount Factor, $ğ›¾$,r)
  : ê°ê°€ìœ¨ì€ ë¯¸ë˜ì— ë°›ì„ ë³´ìƒì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì˜ë¯¸í•œë‹¤.
- ë°˜í™˜ê°’(Return Value, $G_t$)
    > $G_t$ = $R_{t+1}$ + $\gamma R_{t+2}$ + ${\gamma}2R_{t+3}$ +...

- ìƒíƒœ ë³€í™˜ í™•ë¥ ( $P^a_{ss'}$): ìƒíƒœ sì—ì„œ í–‰ë™aë¥¼ í–ˆì„ ë•Œ ë‹¤ìŒ ìƒíƒœê°€ s'ì´ ë˜ëŠ” í™•ë¥ 
- ì •ì±…(Policy, $\pi$): íŠ¹ì • ìƒíƒœ (s)ì—ì„œ ì·¨í•  ìˆ˜ ìˆëŠ” í–‰ë™(a)ë“¤ì— ëŒ€í•œ í™•ë¥  ë¬¸í¬
    > $\pi(a|s)$
- ê°€ì¹˜: ì–´ë–¤ ìƒíƒœì˜ ë°˜í™˜ ê°’ì— ëŒ€í•œ ê¸°ëŒ€ê°’
    * ê°€ì¹˜ê°€ ë†’ë‹¤ -> ì–´ë–¤ ìƒíƒœ sì—ì„œ ë°˜í™˜ ê°’ì— ëŒ€í•œ ê¸°ëŒ€ê°ì´ ë†’ë‹¤.
    * ê°€ì¹˜í•¨ìˆ˜ (Value Function, $V(s)$: íŠ¹ì • ìƒíƒœì— ëŒ€í•œ ê°€ì¹˜ë¥¼ ë„ì¶œí•˜ëŠ” í•¨ìˆ˜
        > $V(s) = E[G_t|S_t=s]$  
        > $V(s) = E[$R_{t+1}$ + $\gamma R_{t+2}$ + ${\gamma}2R_{t+3}|S_t=s]$
    * ìµœì ì˜ í–‰ë™($a^*$): ê°€ì¥ ë†’ì€ ê°€ì¹˜ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” í–‰ë™
        > $a^* = {argmax}_{a\in A} V(s')$

- ë²¨ë§Œ ë°©ì •ì‹
  : í˜„ì¬ ìƒíƒœì˜ ê°€ì¹˜í•¨ìˆ˜ì™€ ë‹¤ìŒ ìƒíƒœì˜ ê°€ì¹˜í•¨ìˆ˜ ì‚¬ì˜ì˜ ê´€ê³„ë¥¼ í‘œí˜„í•œ ë°©ì •ì‹
    > $v_{\pi}(S) = E_{\pi}[R_{t+1} + {\gamma}v_{\pi}(S_{t+1})| S_t = s]$  
    > $q_{\pi}(s,a) = E_{\pi}[R_{t+1} + {\gamma}q_{\pi}(S_{t+1},A_{t+1}| S_t =s, A_t =a]$

- ê°€ì¹˜ ê¸°ë°˜ ê°•í™”í•™ìŠµ
    : ìµœì ì˜ íí•¨ìˆ˜ë¥¼ ì–»ê³  ì´ë¥¼ í† ëŒ€ë¡œ ì˜ì‚¬ê²°ì •í•˜ëŠ” ê²ƒ
    : ìµœì ì˜ í í•¨ìˆ˜ëŠ” ë²¨ë§Œ ìµœì  ë°©ì •ì‹ì„ ì´ìš©í•œ ì‹œê°„ì°¨ ì˜ˆì¸¡ì„ ì´ìš©í•´ ì—…ë°ì´íŠ¸
    > $Q_{k+1}(S_t,A_t) \leftarrow Q_k(S_t,A_t) + \Alpha(R_{t+1} + \gamma \underset{\alpha}{max} Q_k(S_{t+1}, a') - Q_k(S_t,A_t)$
- ì†ì‹¤í•¨ìˆ˜($L(\theta)$)
  