# 강화학습이
## 강화학습 개요
### 머신러닝
머신러닝의 종류는 크게 3가지가 있다. 지도학습, 비지도학습, 강화학습.
  - 지도학습: 어떤 입력과 정답이 있는 데이터 세트를 이용해 학습
  - 비지도학습: 정답이 없는 데이터 세트를 이용해 학습
  - 강화학습: 에이전트와 환경이 상호작용을 하며 보상을 받는 과정에서 생긴 보상을 통해 학습하는것

### MDP(Markov Decision Process) 

### 강화학습의 용어
- 에이전트(agent): 강화학습에서 액션을 결정하는 주체
- 환경(environment): 에이전트가 결정한 액션을 반영하고, 그 정보를 에이전트에 제공
- 행동(action, a): 환경에서 상태를 변화시킬 수 있는 행동
    * 이산적 행동: 여러가지 행동 중 하나를 선택 (상, 하, 좌, 우 중 하나를 선택하는 경우)
    * 연속적 행동: 연속적인 값 중 하나를 선택 (-1과 1 사이의 실수 값)
- 역사
- 상태(state, s): 관측, 행동, 보상을 가공한 정보
- 관측(observation): 환경에서 제공해주는 정보
    * 시각적 관측
    * 수치적 관측
- 보상(reward, $R_t$): 스칼라 값의 피드백. 에이전트는 보상의 합을 최대화 하는것이다. 강화학습은 
- 보상가정(Reward Hypothesis): 기대되는 보상의 누적을 최대화하는것을 목표로 하는것
- 보상함수: 현재 상태 s에서 특정 행동 a를 했을 때 얻는 보상의 기대값 $R^a_s$  
    > $R^a_s = E[R_{t+1} | S_t = s, A_t = a]$

- 감가율(Discount Factor, $𝛾$,r)
  : 감가율은 미래에 받을 보상에 대한 가중치를 의미한다.
- 반환값(Return Value, $G_t$)
    > $G_t$ = $R_{t+1}$ + $\gamma R_{t+2}$ + ${\gamma}2R_{t+3}$ +...

- 상태 변환 확률( $P^a_{ss'}$): 상태 s에서 행동a를 했을 때 다음 상태가 s'이 되는 확률
- 정책(Policy, $\pi$): 특정 상태 (s)에서 취할 수 있는 행동(a)들에 대한 확률 문포
    > $\pi(a|s)$
- 가치: 어떤 상태의 반환 값에 대한 기대값
    * 가치가 높다 -> 어떤 상태 s에서 반환 값에 대한 기대감이 높다.
    * 가치함수 (Value Function, $V(s)$: 특정 상태에 대한 가치를 도출하는 함수
        > $V(s) = E[G_t|S_t=s]$  
        > $V(s) = E[$R_{t+1}$ + $\gamma R_{t+2}$ + ${\gamma}2R_{t+3}|S_t=s]$
    * 최적의 행동($a^*$): 가장 높은 가치를 얻을 수 있는 행동
        > $a^* = {argmax}_{a\in A} V(s')$

- 벨만 방정식
  : 현재 상태의 가치함수와 다음 상태의 가치함수 사의의 관계를 표현한 방정식
    > $v_{\pi}(S) = E_{\pi}[R_{t+1} + {\gamma}v_{\pi}(S_{t+1})| S_t = s]$  
    > $q_{\pi}(s,a) = E_{\pi}[R_{t+1} + {\gamma}q_{\pi}(S_{t+1},A_{t+1}| S_t =s, A_t =a]$

- 가치 기반 강화학습
    : 최적의 큐함수를 얻고 이를 토대로 의사결정하는 것
    : 최적의 큐 함수는 벨만 최적 방정식을 이용한 시간차 예측을 이용해 업데이트
    > $Q_{k+1}(S_t,A_t) \leftarrow Q_k(S_t,A_t) + \Alpha(R_{t+1} + \gamma \underset{\alpha}{max} Q_k(S_{t+1}, a') - Q_k(S_t,A_t)$
- 손실함수($L(\theta)$)
  