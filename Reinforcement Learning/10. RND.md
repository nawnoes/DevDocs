
# RND (Exploration by Random Network Distillation)
OpenAIì—ì„œ ë°œí‘œí•œ Explorationì— ëŒ€í•œ ë…¼ë¬¸. 18ë…„ë„ê¹Œì§€ ì—¬ëŸ¬ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ ìˆì—ˆì§€ë§Œ Rewardê°€ Sparseí•œ í™˜ê²½ì¸ ëª¬í…Œì£¼ë§ˆì˜ ë³µìˆ˜ì—ì„œ `ì¸ê°„ì˜ ì„±ëŠ¥`ì„  ìµœì´ˆë¡œ ë„˜ì€ ë°©ë²•ì´ë‹¤. 

![](https://images.velog.io/images/nawnoes/post/eb515fa0-1747-461f-9781-679f16effeb9/image.png)
> ëª¬í…Œì£¼ë§ˆì˜ ë³µìˆ˜ ê²Œì„ì˜ ëª¨ìŠµ

## Abstract
- ì´ ë…¼ë¬¸ì—ì„œ ê°•í™”í•™ìŠµì„ ìœ„í•œ Exploration bonusë¥¼ ì†Œê°œí•œë‹¤.
- ì´ Exploration bonusëŠ” ì ìš©í•˜ê¸° ì‰½ê³ , ë™ì‘í•˜ëŠ”ë° ìµœì†Œì˜ ì˜¤ë²„í—¤ë“œê°€ ë“ ë‹¤.
- ì´ë•Œ bonusëŠ” ê³ ì •ë˜ê³  ëœë¤í•˜ê²Œ ì´ˆê¸°í™”ëœ ë„¤íŠ¸ì›Œí¬ë¡œë¶€í„° ì–»ì€ ê´€ì¸¡(observations)ì˜ íŠ¹ì§•(feature)ì„ ì˜ˆì¸¡í•˜ëŠ” ì‹ ê²½ë§ì˜ ì—ëŸ¬ë¥¼ ë§í•œë‹¤. 
- `intrinsic` and `extrinsic` rewardë¥¼ ì ì ˆí•˜ê²Œ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•œ flexibilityì— ëŒ€í•œ methodë¥¼ ì†Œê°œí•œë‹¤
- ì´ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ëª¬í…Œì£¼ë§ˆì˜ ë³µìˆ˜ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ê³  ì¸ê°„ì˜ ì„±ëŠ¥ì„ ë›°ì–´ë„˜ê²Œ ëœ ì²«ë²ˆì§¸ ë°©ë²•ìœ¼ë¡œ ì˜ì˜ë¥¼ ê°€ì§„ë‹¤.


## 1. Introduction
### ê¸°ì¡´ì˜ ì œí•œ ì‚¬í•­ë“¤
- ê°•í™”í•™ìŠµì€ Policyì˜ ë°˜í™˜ê°’(return)ì˜ ê¸°ëŒ“ê°’ì„ ê·¹ëŒ€í™” í•˜ë„ë¡ ë™ì‘í•œë‹¤. ë³´ìƒ(reward)ê°€ ë°€ì§‘ëœ(dense)í•œ ê²½ìš°, ëŒ€ì²´ì ìœ¼ë¡œ ì˜ í•™ìŠµí•œë‹¤. **í•˜ì§€ë§Œ** ë³´ìƒì´ ëœ¨ë¬¸ëœ¨ë¬¸í•œ(sparse) ê²½ìš°, í•™ìŠµì‹œí‚¤ê¸° ì–´ë µë‹¤.
- ì‹¤ì œ ì„¸ê³„ì—ì„œë„ ëª¨ë“  ë³´ìƒë“¤ì´ ë°€ì§‘ëœ ê²½ìš°ëŠ” ë“œë¬¼ë‹¤. ë”°ë¼ì„œ ë³´ìƒì´ ë“œë¬¸ ê²½ìš°ì—ë„ ì˜ í•™ìŠµí•˜ê¸° ìœ„í•œ ë°©ë²•ì´ í•„ìš”í•˜ë‹¤. 
- 18ë…„ ë‹¹ì‹œ RLì—ì„œëŠ” ì–´ë ¤ìš´ ë¬¸ì œë“¤ì„ í’€ê¸°ìœ„í•´ í™˜ê²½ì„ ë³‘ë ¬ì ìœ¼ë¡œ í¬ê²Œ ë§Œë“¤ê³  ê²½í—˜ë“¤ì„ ìƒ˜í”Œë§í•˜ì—¬ ì–»ì—ˆì§€ë§Œ ì´ëŸ¬í•œ ë°©ë²•ì€ ë‹¹ì‹œì— ì†Œê°œëœ exploration ë°©ë²•ë“¤ì— ëŒ€í•´ì„œëŠ” ì ìš©í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆì—ˆë‹¤. 

### ì œì‹œí•˜ëŠ”ê²ƒ
ê°„ë‹¨í•˜ê³  ê³ ì°¨ì›ì˜ ê´€ì¸¡ì—ì„œë„ ì˜ ë™ì‘í•˜ëŠ” Exploration bonusë¥¼ ì†Œê°œí•œë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì€ policy optimizationê³¼ í•¨ê»˜ ì ìš©í•  ìˆ˜ë„ ìˆê³ , ê³„ì‚° íš¨ìœ¨ì ì´ê¸°ë„ í•˜ë‹¤. 

### ì•„ì´ë””ì–´
ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ **Exploration bonus**ëŠ” ì‹ ê²½ë§ì—ì„œ í•™ìŠµëœ ê²ƒë“¤ê³¼ ìœ ì‚¬í•œ ì˜ˆë“¤ì— ëŒ€í•´ì„œ `prediction error`ê°€ í˜„ì €í•˜ê²Œ ë‚®ì€ ê²ƒì—ì„œ ì•„ì´ë””ì–´ë¥¼ ì–»ì—ˆë‹¤.  
  
ìƒˆë¡œìš´ ê²½í—˜ë“¤ì˜ ì°¸ì‹ í•¨ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´, ì—ì´ì „íŠ¸ì˜ ê³¼ê±° ê²½í—˜ë“¤ì— ëŒ€í•´ í•™ìŠµí•œ ë„¤íŠ¸ì›Œí¬ì˜ `prediction error`ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. 

### prediction errorë¥¼ ê·¹ëŒ€í™” í•˜ëŠ”ê²ƒì˜ ë¬¸ì œ
ì´ì „ì— ë§ì€ ì €ìë“¤ì´ ì§€ì í–ˆë“¯ì´ `prediction error`ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ì—ì´ì „íŠ¸ì˜ ê²½ìš°ëŠ” prediction problemì˜ ë‹µì´ ì…ë ¥ì˜ í™•ë¥ ì  í•¨ìˆ˜ì¸ transitionë“¤ì— ëŒ€í•´ ëŒë¦¬ê²Œ ëœë‹¤.  
  
ì˜ˆë¥¼ë“¤ì–´ Noise TV ì™€ ê°™ì€ ì˜ˆê°€ ìˆë‹¤. í˜„ì¬ obsevationì—ì„œ ë‹¤ìŒ observationì„ ì˜ˆì¸¡í•˜ëŠ” prediction problemì´ ìˆê³ ,  ì´ë•Œ ì—ì´ì „íŠ¸ê°€ `prediction error`ë¥¼ ê·¹ëŒ€í™” í•˜ë„ë¡ í–‰ë™í•œë‹¤ë©´, í™•ë¥ ì  ì „í™˜(stochastic transitions)ì„ ì°¾ë„ë¡ í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. 
> ê·¸ ì˜ˆë¡œ ë¬´ì‘ìœ„ë¡œ ë³€í•˜ëŠ” ì•„ë˜ì™€ ê°™ì€ í™”ë©´ì¸ Noise TVì™€ ë¬´ì‘ìœ„ë¡œ ë™ì „ì„ ë˜ì§€ëŠ” ì´ë²¤íŠ¸ì™€ ê°™ì€ ê²ƒë“¤ì´ ìˆë‹¤. 

![](https://cdn.vox-cdn.com/thumbor/Tb-Wu0JnAh_oQARUbmTpOHRodvA=/800x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/13369609/noisy_tv_problem.gif)

### ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ëŒ€ì•ˆ
ë…¼ë¬¸ì˜ ì €ìë“¤ì€ ì ì ˆí•˜ì§€ ì•Šì€ stochasticityì— ëŒ€í•œ ëŒ€ì•ˆì„ ì œì‹œí•œë‹¤. ë‹µì´ ê·¸ê²ƒì˜ ì…ë ¥ì— deterministic functionì¸ prediction problemì„ exploration bonusìœ¼ë¡œ ì •ì˜í•˜ë©´ì„œ ëŒ€ì•ˆìœ¼ë¡œ ì œì•ˆí•œë‹¤.  

ëŒ€ì•ˆìœ¼ë¡œëŠ”, í˜„ì¬ Observationì— ëŒ€í•´ì„œ outputì„ `fixed randomly initialized network`ë¥¼ ì‚¬ìš©í•´ì„œ ì˜ˆì¸¡í•œë‹¤ëŠ” ê²ƒì´ë‹¤. 

### ê°•í™”í•™ìŠµì—ì„œ ì–´ë ¤ìš´ ê²Œì„ë“¤ ğŸ¤¦â€â™‚ï¸
Bellemareì˜ 2016ë…„ ë…¼ë¬¸ì—ì„œ ë³´ìƒì´ ë“œë¬¼ê³  íƒìƒ‰ì´ ì–´ë ¤ìš´ ê²Œì„ë“¤ì„ í™•ì¸í–ˆë‹¤. ê·¸ ê²Œì„ë“¤ë¡œëŠ” `Freeway`, `Gravitar`, `Montezuma's Revenge`, `Pitfall`, `Private Eye`, `Solaris`, `Venture` ë“¤ì´ ìˆê³ , ì–´ë–¤ ê²½ìš°ì—ì„œëŠ” ë‹¨ í•˜ë‚˜ì˜ ê¸ì •ë³´ìƒë„ ì°¾ì§€ ëª»í–ˆë‹¤.  
  
### ê·¸ ì¤‘ì—ì„œë„ ì–´ë ¤ìš´ ëª¬í…Œì£¼ë§ˆì˜ ë³µìˆ˜
![](https://images.velog.io/images/nawnoes/post/a8da85ed-c308-4bca-8130-4cb56047f055/image.png)  
ëª¬í…Œì£¼ë§ˆì˜ ë³µìˆ˜ëŠ” ê°•í™”í•™ìŠµì—ì„œ ì–´ë ¤ìš´ ê²Œì„ìœ¼ë¡œ ì—¬ê²¨ì§„ë‹¤. ê²Œì„ì—ì„œ ì¹˜ëª…ì ì¸ ì¥ì• ë¬¼ì„ í”¼í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ê²Œì„ìŠ¤í‚¬ë“¤ì— ëŒ€í•œ ì¡°í•©ë“¤ì´ í•„ìš”í•˜ê³ , ìµœì ì˜ í”Œë ˆì´ë¥¼ í•˜ë©´ì„œë„ ë³´ìƒì´ ìˆ˜ë°± ìŠ¤í… ì´ìƒ ë–¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì—, ë³´ìƒì„ ì°¾ê¸°ì— ì–´ë ¤ìš´ ê³¼ì œë¡œ ì—¬ê²¨ì§„ë‹¤.  
  
ëª‡ ë…¼ë¬¸ë“¤ì—ì„œ ê½¤ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆì§€ë§Œ, expert demonstrationsì™€ emulator stateì— ëŒ€í•œ ì ‘ê·¼ ì—†ì´ëŠ” ê°€ì¥ ì¢‹ì€ ì„±ê³¼ê°€ ì „ì²´ ë°©ì˜ ì ˆë°˜ë§Œ ì°¾ì•„ë‚´ì—ˆë‹¤.

### ë…¼ë¬¸ì—ì„œ ë°œê²¬í•œê²ƒ ğŸ”
#### extrinsic reward ì—†ëŠ” ê²½ìš°
- `extrinsic reward`ë¥¼ ë¬´ì‹œí•˜ì—¬ë„ agentê°€ **RND exploration bonus**ë¥¼ ê·¹ëŒ€í™” í•˜ë„ë¡ í•œë‹¤ë©´ ì¼ê´€ë˜ê²Œ ëª¬í…Œì£¼ë§ˆì˜ ë³µìˆ˜ ê²Œì„ì—ì„œ ì ˆë°˜ ì´ìƒì˜ ë°©ì„ ì°¾ì•˜ë‹¤. 
#### extrinsic rewardì™€ exploration bonus ê²°í•©
- exploration bonusì™€ `extrinsic reward`ë¥¼ ê²°í•©í•˜ê¸° ìœ„í•´ì„œ ìˆ˜ì •ëœ PPO(Poximal Policy Optimization)ì„ ì œì•ˆí•œë‹¤. ìˆ˜ì •ëœ PPOëŠ” 2ê°œì˜ ë³´ìƒì„ ìœ„í•´ 2ê°œì˜ `value head`ë¥¼ ì‚¬ìš©í•œë‹¤. 
- ì„œë¡œ ë‹¤ë¥¸ ë³´ìƒ(reward)ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ discount rateë¥¼ ì‚¬ìš©í•˜ê³ , `episodic`ê³¼`non-episodic` ë°˜í™˜ê°’(return)ì„ ê²°í•©í•œë‹¤. 

### ì„±ê³¼
**first levelì—ì„œ** ì¢…ì¢… ëª¬í…Œì£¼ë§ˆ ê²Œì„ì—ì„œ 24ê°œì˜ ë°© ì¤‘ì—ì„œ 22ê°œë¥¼ ì°¾ì•˜ê³ , ì´ë”°ê¸ˆì”© í´ë¦¬ì–´ í•˜ê¸°ë„ í–ˆë‹¤. ê·¸ë¦¬ê³  ê°™ì€ ë°©ë²•ìœ¼ë¡œ Ventureì™€ Gravitarì—ì„œ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.   
**Venture**  
![](https://images.velog.io/images/nawnoes/post/87b87d1e-0ac4-4bf3-9a70-d65e61e64586/image.png)
  
**Gravitar**  
![](https://images.velog.io/images/nawnoes/post/6333fdd9-ae28-465a-b7b8-fd32dc0ec177/image.png)
## 2. Method

### 2.1 Exploration Bonuses
explorration bonusëŠ” ë³´ìƒ $e_t$ê°€ ë“œë¬¸ í™˜ê²½(environment)ì—ì„œë„ agentê°€ exploration í• ìˆ˜ ìˆë„ë¡ ì¥ë ¤í•˜ëŠ” ë°©ë²•.
- ë³´ìƒ $e_t$ëŠ” ìƒˆë¡œìš´ ë³´ìƒ $r_t= e_t + i_t$ë¡œ ëŒ€ì²´ 
- ì´ ë•Œ, $i_t$ëŠ” exploration bonusë¡œ time $t$ì˜ transitionê³¼ ì—°ê´€ìˆë‹¤. 

#### ì¥ë ¤ë¥¼ ìœ„í•œ ë°©ë²•
agentê°€ novel stateë“¤ì„ íƒìƒ‰í•˜ë„ë¡ ì¥ë ¤í•˜ëŠ” ë°©ë²•ì€, ìì£¼ ë°©ë¬¸í•˜ëŠ” ê²ƒë“¤ë³´ë‹¤ novel stateë“¤ì— ëŒ€í•´ $i_t$ ê°’ì´ ë” ì»¤ì§€ë„ë¡ í•œë‹¤.
##### 1) Count-Based
ìƒíƒœì— ëŒ€í•œ ì œí•œ ìˆ˜ë¥¼ ê°€ì§„ **tabular setting**ì—ì„œëŠ” $i_t$ ë¥¼ ë°©ë¬¸ìˆ˜ $n_t$ì— ë”°ë¼ ì ì  ì¤„ì–´ë“¤ë„ë¡ ì •ì˜ í• ìˆ˜ ìˆê³  ì´ì „ ë…¼ë¬¸ë“¤ì—ì„œ ì‚¬ìš© ë˜ì—ˆë‹¤. 
- $i_t = 1/n_t(s)$
- $i_t = 1/\sqrt{n_t(s)}$

**non-tabular case**ì—ì„œëŠ” ëŒ€ë¶€ë¶„ì˜ ìƒíƒœë“¤ì´ í•œë²ˆ ì •ë„ë§Œ ë°©ë¬¸í•˜ê¸° ë•Œë¬¸ì— ìœ„ì™€ ê°™ì´ ì ìš©í•˜ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šë‹¤. ê·¸ë˜ì„œ pseudo-countì™€ ê°™ì€ ë°©ë²•ë“¤ì´ ìˆë‹¤. pseudo-countëŠ” **ì´ì „ì— ë°©ë¬¸í–ˆë˜ ìƒíƒœë“¤ê³¼ ìœ ì‚¬í•˜ë‹¤ë©´ ê³¼ê±°ì— ë°©ë¬¸í•œì ì´ ì—†ëŠ” ìƒíƒœë“¤ì— ëŒ€í•´ ì–‘ìˆ˜ì¸ ê°’ì„** ì¤„ ìˆ˜ ìˆëŠ” `density model`ë¡œë¶€í„° ì–»ì„ ìˆ˜ ìˆë‹¤.  
  
 
### 2.2 Random Network Distollation
ì´ ë…¼ë¬¸ì—ì„œëŠ” ëœë¤í•˜ê²Œ ìƒì„±ëœ prediction problemì— ëŒ€í•œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²•ì„ ì†Œê°œ í•œë‹¤.`target network`ì™€ `predictor network`ì™€ ê°™ì´ **2ê°œì˜ ë„¤íŠ¸ì›Œí¬**ë¥¼ ì‚¬ìš©í•œë‹¤. 
- $target$ network: `fixed random initialized target network` , prediction problem set
- $predictor$ network: agentì— ì˜í•´ ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” ë„¤íŠ¸ì›Œí¬

**target network**  
$f: O {\rightarrow} R^k$

**predictor network**  
$\hat{f}: O {\rightarrow} R^k$

ìœ„ì™€ ê°™ì´ êµ¬í•˜ê³  $||\hat{f}(x;\theta)-f(x) ||^2$ ì—ëŸ¬ê°’ì„ ìµœì†Œí™” í•˜ë„ë¡ ê·¸ë˜ë””ì–¸íŠ¸ ë””ì„¼íŠ¸ ë°©ë²•ì„ í†µí•´ predictorì˜ íŒŒë¼ë¯¸í„° $\theta_{\hat{f}}$ì„ í•™ìŠµí•œë‹¤.    
   

***ìœ„ ë°©ë²•ì´ target networkì¸ randomly initialized neural networkë¥¼ ì´ìš©í•´ì„œ predictor networkë¥¼ í•™ìŠµ ì‹œí‚¤ëŠ” ê³¼ì •ì´ë‹¤.***
  
  
prediction errorëŠ” predictorê°€ í•™ìŠµí•œê²ƒê³¼ ë‹¤ë¥¸ novel stateì— ëŒ€í•´ì„œ ë†’ì€ ê°’ì„ ê°€ì§€ê²Œ ë ê²ƒì´ë‹¤. 
![](https://images.velog.io/images/nawnoes/post/662b1076-29a7-4a86-ab76-dbc1e819f338/image.png)

ìœ„ figure2ì—ì„œëŠ” target classì—ì„œ training exampleë“¤ì˜ ìˆ˜ì— ëŒ€í•´ test errorrê°€ ê°ì†Œí•˜ëŠ”ê²ƒì„ ë³´ì—¬ì£¼ê³  novel stateë¥¼ íƒì§€í•  ìˆ˜ ìˆëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŒì„ ì•Œë ¤ì¤€ë‹¤.  
> predictorê°€ ëœë¤í•œ target networkë¥¼ ì™„ì „íˆ ëª¨ì‚¬í•  ìˆ˜ ìˆì„ê±°ë¼ëŠ” ë°˜ëŒ€ ì˜ê²¬ë„ ìˆì§€ë§Œ ìœ„ figureë¥¼ í†µí•´ ì•„ë‹˜ì„ ë³´ì˜€ë‹¤. 

#### 2.2.1 Source of Prediction Errors
ì¼ë°˜ì ìœ¼ë¡œ prediction error ë“¤ì€ ì•„ë˜ ìš”ì†Œë“¤ë¡œë¶€í„° ë‚˜ì˜¨ë‹¤.
##### 1. Amount of training data  
predictorê°€ ì „ì— í•™ìŠµí•œ exampleë“¤ê³¼ ë‹¤ë¥¼ìˆ˜ë¡ prediction errorê°€ ë†’ë‹¤
##### 2. Stochasticity
target functionì´ í™•ë¥ ì ì´ê¸° ë•Œë¬¸ì— prediction errorê°€ í¬ë‹¤. stochastic transitionë“¤ì€ forward dynamic predictionê³¼ ê°™ì€ ì—ëŸ¬ë“¤ì˜ ì›ì¸ì´ë‹¤.
##### 3. Model misspecification
í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ëª¨ë¸ì´ target funtionì˜ ë³µì¡ë„ì— ë§ì¶”ê¸° ìœ„í•´ ì œí•œì ì¸ ê²½ìš° 
##### 4. Learning dynamics
predictorê°€ target funtionì„ ê·¼ì‚¬ í•˜ëŠ” ê³¼ì •ì—ì„œ ìµœì í™”ë¥¼ ì‹¤íŒ¨í–ˆì„ë•Œ, 

- 2ë²ˆì§¸ ìš”ì†ŒëŠ” noisy TV ë¬¸ì œë¥¼ ì•¼ê¸°í•˜ê³  3ë²ˆì§¸ ìš”ì†Œë„ ì ì ˆí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— RNDì—ì„œëŠ” ìš”ì†Œ 2,3ì„ ì œê±°í•œë‹¤. 3 since the target network can be chosen to be deterministic and inside
the model-class of the predictor network. 
#### 2.2.2 Relation to Uncertaionty Quantification
RNDì˜ prediction errrorëŠ” ë¶ˆí™•ì‹¤í•œê²ƒì„ ìˆ˜ëŸ‰í™”í•˜ëŠ” ê²ƒê³¼ ê´€ë ¨ì´ ìˆë‹¤.  

**RND prediction error**  
- Data distribution $D={x_i,y_i}_i$ì— ëŒ€í•œ regression ë¬¸ì œë¡œ ë³¼ìˆ˜ ìˆë‹¤.
- ë² ì´ì§€ì•ˆ ì„¤ì •ì—ì„œ ë§¤í•‘ëœ $f_{\theta}$ì˜ íŒŒë¼ë¯¸í„°ë“¤ì— ëŒ€í•œ ì´ì „ $P(\theta^*)$ë¥¼ ê³ ë ¤í•˜ê³  evidence ë“¤ì— ëŒ€í•´ ì—…ë°ì´íŠ¸ í•œ í›„ì— ë’¤ìª½ì„ ê³„ì‚°í•œë‹¤. 
- $p({\theta})$ë¡œ ë¶€í„° ë‚˜ì˜¨ ${\theta}^*$ì— ëŒ€í•´ í•¨ìˆ˜ $g_{\theta}=f_{\theta}+f_{\theta^*}$ì˜ ë¶„í¬ê°€ ë˜ëŠ” F
- ${\theta}$ëŠ” ê¸°ëŒ€ë˜ëŠ” prediction errorë¥¼ ìµœì†Œí™” í•œë‹¤
- ${\theta} = \underset{\theta}{argmin}E_{(x_i,y_i)~D}||f_{\theta}(x_i)+f_{\theta^*}(x_i) -y_i||^2 + R({\theta})$ 
- $R({\theta})$ ëŠ” ì´ì „ ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¨ ì •ê·œí™” ìš©ì–´. OsbandsëŠ” 18ë…„ ëˆˆë¬¸ì—ì„œ ì•™ìƒë¸” Fê°€ ìˆ˜ì‹ì— ëŒ€í•œ ê·¼ì‚¬ë¼ê³  í•¨.

**regression target $y_i$ê°€ 0ì´ ëœë‹¤ë©´**  
- ìµœì í™” ë¬¸ì œëŠ” $E_{(x_i,y_i)~D}||f_{\theta}(x_i)+f_{\theta^*}(x_i)||^2$ëŠ” distilling a randomly drawn functionê³¼ ë™ì¼ í•˜ë‹¤ê³  ë³¼ìˆ˜ ìˆë‹¤. 
- ì´ëŸ° ê´€ì ì—ì„œ `predictor`ì™€ `target` networkì˜ ê²°ê³¼ì˜ ê° ì¢Œí‘œë“¤ì€ ì•™ìƒë¸”ì˜ ë©¤ë²„ë¡œ ë°˜ì‘í•˜ë©° MSE ëŠ” ì•™ìƒë¸”ì˜ ì˜ˆì¸¡ëœ varianceê°€ ë  ìˆ˜ ìˆë‹¤. 
- ìœ„ ê³¼ì •ì„ í†µí•´ distillation error ë¶ˆí™•ì„±ì— ëŒ€í•œ ê³„ëŸ‰ì„ í•  ìˆ˜ ìˆìŒì„ ë³´ì¸ë‹¤. 
### 2.3 Combining Intrrinsic and Extrinsic Return
ì´ì „ ì‹¤í—˜ë“¤ì—ì„œ `intrinsic reward`ë§Œ non-episodic ë¬¸ì œì—ì„œ ë‹¤ë£¨ì—‡ìœ¼ë©°, ë” ë‚˜ì€ exploration ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì—ì„œëŠ” ê²Œì„ì´ ì˜¤ë²„ë˜ì–´ë„ ë¦¬í„´ì´ ì¢…ë£Œ ë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ë¬¸ì œê°€ ìˆì—ˆë‹¤. 
- ì €ìë“¤ì€ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ë” ìì—°ìŠ¤ëŸ¬ìš´ exploration ë°©ë²•ì„ ë³´ì¸ë‹¤. 
- agent ì˜ `intringic return`ì€ ë¯¸ë˜ì— ì°¾ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  novel stateë“¤ì— ëŒ€í•´ ê´€ë ¨ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—.
- ê·¸ë¦¬ê³  ì´ ë°©ë²•ì´ ì¸ê°„ì´ ê²Œì„ì—ì„œ explore í•˜ëŠ” ë°©ë²•ê³¼ ê°€ê¹ë‹¤ê³  ì£¼ì¥í•œë‹¤.

**extrrinsic rewardì— ëŒ€í•œ non-episodic return**
non-episodic return ì— ëŒ€í•´ non-episodic returnì„ ì‚¬ìš©í•˜ëŠ”ê²ƒì€ ì „ëµì´ ê²Œì„ì˜ ì‹œì‘ê³¼ ê°€ê¹Œìš´ ë³´ìƒì´ ì´ìš©(exploited)ë˜ë„ë¡ í•  ìˆ˜ ìˆìœ¼ë©°, ê³ ì˜ì ìœ¼ë¡œ ê²Œì„ì„ ì¢…ë£Œì‹œí‚¤ê³  ë‹¤ì‹œ ê²Œì„ì˜ ì‹œì‘ìœ¼ë¡œ ëŒì•„ê°€ëŠ” ì‚¬ì´í´ì´ ë°˜ë³µ ë  ìˆ˜ ìˆë‹¤.  
  
**non-episodic stream valueì—ì„œ intrinsic reward $i_t$ì™€ epsodic streamì—ì„œ extrinsic reward $e_t$ì˜ ì¡°í•©ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì€ ë¶ˆë¶„ëª… í•˜ë‹¤.**  
- ê·¸ë˜ì„œ ì €ìë“¤ì˜ ë°©ë²•ì€ returnë“¤ì´ linearì¸ì§€ ê´€ì°°í•œë‹¤. ì´ë•Œ $R = R_E+R_I$ì„ returnìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.
- ë”°ë¼ì„œ 2ê°œì˜ value í—¤ë“œ $V_E$ì™€ $V_I$ ê°’ì„ ê°ê°ì˜ returnë“¤ì„ ì‚¬ìš©í•´ì„œ fit í• ìˆ˜ ìˆê³ 
- ìœ„ì˜ ê°’ì„ ì¡°í•©í•œ $V=V_E+V_I$ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

### 2.4 Reward and Observation Normalization 
#### reward
prediction errorë¥¼ exploration bonusë¡œ ì‚¬ìš©í•˜ëŠ”ê²ƒì´ ë³´ìƒì˜ í¬ê¸°ê°€ í™˜ê²½ì— ë”°ë¼, ì‹œê°„ì— ë”°ë¼ í¬ê²Œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ëª¨ë“  í™˜ê²½ì—ì„œ ë™ì‘í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ë¶€ë¶„ì´ ìˆë‹¤.  
  
ì¼ì •í•œ í¬ê¸°ì˜ rewardë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ `intrinsic reward`ë¥¼ `intrinsic reward`ì˜ í‘œì¤€í¸ì°¨ë“¤ì— ëŒ€í•œ ì¶”ì •ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì£¼ì–´ normalization ê³¼ì •ì„ ê±°ì¹œë‹¤. 
#### observation
ëœë¤ ì‹ ê²½ë§ì„ ì‚¬ìš©í•  ë•ŒëŠ” íŒŒë¼ë¯¸í„°ë“¤ì´ ê³ ì •ë˜ê³  ë‹¤ë¥¸ ë°ì´í„° ì…‹ë“¤ì— ëŒ€í•´ì„œë„ ì¡°ì •ë˜ë©´ ì•ˆë˜ê¸° ë•Œë¬¸ì— observation normalizationì´ ì¤‘ìš”í•˜ë‹¤. normalizationì´ ë¶€ì¡±í•˜ë©´ ì„ë² ë”©ì˜ ë¶„ì‚°ì´ ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì•„ì§€ê³  ì…ë ¥ì— ëŒ€í•œ ì •ë³´ê°€ ì¡°ê¸ˆë§Œ ì „ë‹¬ë˜ëŠ” ê²°ê³¼ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.   
- ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ continuous control ë¬¸ì œì—ì„œ ì‚¬ìš©í•˜ëŠ” observation normalization schemeì„ ì‚¬ìš©í•˜ê³ , ê° dimensionì— running meanì„ ë¹¼ê³  running standard deviationì„ ë‚˜ëˆ„ì–´ ê°’ì„ normalization.  
- ê·¸ í›„ ê°’ì´ -5ì™€ 5ì‚¬ì´ì— ì˜¤ë„ë¡ ì˜ë¼ì¤€ë‹¤. 
- predictorì™€ target networkëŠ” ë™ì¼í•œ obsevation nomalization, policy networkì— ëŒ€í•´ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
## References
- https://arxiv.org/pdf/1810.12894.pdf
- https://kr.endtoend.ai/slowpapers/rnd
- https://bluediary8.tistory.com/37
- https://seolhokim.github.io/deeplearning/2019/10/11/Exploration-by-random-network-distillation-review/
- https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/