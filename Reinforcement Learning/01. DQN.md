# Deep Q Leaning
Neural network를 이용한 function approxiamtor로 기존의 딥러닝 방식의 강화학습이 가진 문제를
`Experience Replay Buffer`와 `Fixed Q Targets`을 이용해 해결하였다.
## Q-Leanring이란

![](https://perfectial.com/wp-content/uploads/2019/10/q_learning-01.jpg)

모델이 없는 (Model-Free) 환경에서 학습하는 알고리즘. Agent가 특정 state에서 특정 action을 하도록 하는 Policy를 학습하것을 말한다.현재 state에서 부터 시작해 연속적인 단계들에 대한 전체 보상의 기대값을 최대화 한다.  이러한 개념은 한 state에서 다른 state에서의 transition이 확률적으로 일어나거나 보상이 확률적으로 주어지는 환경에서도 적용 가능하다. 
 
### Q-table

![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Q-Learning_Matrix_Initialized_and_After_Training.png/440px-Q-Learning_Matrix_Initialized_and_After_Training.png)

모든 상태와 행동에 대한 기록을 Q-table에 담는다. 하지만 많은 상태와 행동이 존재하는 환경에서는 학습에 어려움이 있다. 
  
> Q는 현재 상태에서 취한 행동의 보상에 대한 Quailty를 뜻한다.  

### Q-Value
상태 s에서 행동 a를 했을때 그 행동의 가치는 $Q(s,a)$로 나타낸다. 이때 Q-value는시간 t에서 정책$\pi$를 따라 행동a를 할 때 미래 보상들의 총합의 기대값.

$Q_{\pi} = E_{\pi}[R_{t+1} + {\gamma}R_{t+2}+ ... +|S_t =s, A_t=a]$

## DQN
Q-learning에서 Function Approximator로 Deep Learning을 이용한 방법. 기존 딥러닝 기반의 Q러닝이 가진 단점들을 `Experience Replay`와 `Fixed Q Targets`을 통해 극복하였다

### 1. Experience Replay Buffer
#### 문제점
- 기존의 Q-러닝에서는 관측치를 생성하고, 바로 Q 함수를 업데이트 하는 과정을 반복해서 학습했다. 이전의 관측치가 다음 관측치에 영향을 주어
학습이 잘 안되는 문제.
- 기존 Q-러닝에서의 문제는 바로 데이터가 버려졌었다. 학습에 도움이 되는 데이터가 한번 사용되고 버려지기 때문에
비효
> uniform random sampling을 통해 관측값을 추출하므로 데이터간 상관 관계가 사라진다.
> Replay Buffer에 관측값을 담아 두어 진행한 데이터들이 사라지지 않고, 재사용 된다. 율적인 부분 존재.

#### 해결 방법
- Experience Replay를 사용하는 agent는 관측 값을 얻을 때마다 이를 Replay Buffer에 저장. 
- 저장된 버퍼 (history)에서 배치 만큼 샘플링하여, Q-러닝 업데이트 사용

### 2. Fixed Q Targets
- Q 함수의 Target = $( r + max_{a'}Q(s',a'))$
#### 문제점
기존에 타겟으로 추정치 Q값을 예측할때, 추정치Q 로 Nueral Network를 업데이트하면, 타겟에서 Q 값의 웨이트도 같이 업데이트 되므로, 타겟도 변하게 된다.
이때 추정치 Q를 타겟에 가깝게 오차를 줄여하는데, 타겟의 웨이트도 같이 변경 되게 되면 학습이 효율적으로 되지 않는다.
#### 해결
target network의 업데이트 주기를 local network보다 더 느리가 만든다. 예를 들어 DQN 논문에서는 local network가 4번 업데이트 될때,
target network를 한번 local network의 파라미터로 업데이트한다.  
  



# References
https://perfectial.com/blog/q-learning-applications/



